{
  "id": "disc-1771432148704",
  "title": "I want to expand the locus services to launch an AWS instance so that people can launch their agent server and add integrations. What do you think?",
  "topic": "I want to expand the locus services to launch an AWS instance so that people can launch their agent server and add integrations. What do you think?",
  "status": "active",
  "messages": [
    {
      "role": "assistant",
      "content": "Hey! I'm here to help you think through this expansion. Launching AWS instances for agent server hosting is a significant infrastructure move, so let's make sure we nail the details.\n\nLet me start with the core question: **What's the primary user problem you're solving here?** Is this about giving users a managed environment so they don't have to figure out hosting themselves, or is it more about providing a standardized runtime where integrations (MCP servers, tools, etc.) can reliably connect to the agent?",
      "timestamp": 1771432158032
    },
    {
      "role": "user",
      "content": "I mean it is difficult to launch your own server, clone your working repo there, install claude or codex, then manage the integration versions, etc etc. We have an easy installation script but it is still difficult to launch if you don't know much about AWS, Azure and other services.",
      "timestamp": 1771432361593
    },
    {
      "role": "assistant",
      "content": "That makes a lot of sense. So the core value prop is **removing the infrastructure complexity** — users shouldn't need to know AWS/Azure to get a running agent server with their repo and integrations ready to go.\n\nHere's what I want to understand next: **Who is the target user for this?** Are you primarily thinking about individual developers who want a persistent agent running on their personal repos, or is this aimed at teams/companies who want to run Locus as a shared service (like a CI/CD-style agent that picks up tasks from multiple team members)?",
      "timestamp": 1771432372590
    },
    {
      "role": "user",
      "content": "Yes, individual developers in the beginning.",
      "timestamp": 1771432404481
    },
    {
      "role": "assistant",
      "content": "Good, starting with individual developers keeps the scope manageable and lets you iterate on the experience before tackling team-level complexity.\n\nNow, here's a key architectural question: **What's the lifecycle of these instances?** Are you thinking always-on persistent servers that stay running (like a VPS), or ephemeral instances that spin up on demand when a task comes in and shut down when idle — more like a serverless/spot-instance model? This has huge implications for cost (which the user pays), complexity, and how integrations stay configured.",
      "timestamp": 1771432413713
    },
    {
      "role": "user",
      "content": "Not shure about it, what do you recommend. The goal is simple, it should listen to user requests from telegram or future integrations and then execute commands locally and then push the changes. I think it should be persistent because otherwise how we will manage the hosting repository there?",
      "timestamp": 1771432497075
    },
    {
      "role": "assistant",
      "content": "You're right that persistent makes more sense for this use case. The repo needs to stay cloned, the agent needs to be listening for messages continuously, and rebuilding the environment on every request would add unacceptable latency. The cost trade-off is that users pay for uptime even when idle, but for individual developers running a personal agent, a small always-on instance (like a `t3.micro` or `t3.small`) is only ~$8-15/month — very reasonable.\n\nHere's the next important question: **How do you envision the provisioning flow from the user's perspective?** For example, does the user click a button in the Locus web app, provide their GitHub repo URL and API keys (Claude/OpenAI), and then Locus handles everything behind the scenes? Or are you thinking the user connects their own AWS account (so the instance runs in *their* infrastructure and they pay AWS directly), and Locus just orchestrates the setup? This is a big distinction — it determines whether you're becoming a hosting provider or an infrastructure orchestrator.",
      "timestamp": 1771432511446
    },
    {
      "role": "user",
      "content": "Which one do you recommend?",
      "timestamp": 1771432556785
    },
    {
      "role": "assistant",
      "content": "Great question — both models have real trade-offs, so let me give you a clear recommendation.\n\n**I'd recommend starting as an infrastructure orchestrator** — where Locus provisions the instance in the user's own AWS account. Here's why:\n\n1. **You avoid becoming a hosting provider**, which comes with billing complexity, abuse risk, support burden for infrastructure issues, and liability for uptime. That's a whole business to run on top of your actual product.\n2. **Users pay AWS directly**, so you don't need to handle payment processing for compute costs, manage margins, or deal with cost disputes.\n3. **Trust is easier** — developers are more comfortable knowing their code and API keys live in *their* AWS account, not on someone else's infrastructure.\n4. **The UX can still be one-click**: the user connects their AWS account via an IAM role or access keys once, and from then on Locus handles all the provisioning, setup, and management transparently.\n\nThe downside is a slightly more complex onboarding (the user needs an AWS account and has to grant access), but for individual developers — who are your initial target — most already have one or can create one in minutes.\n\nNow here's the next thing I want to drill into: **What's the minimum set of configuration the user needs to provide to get a working instance?** I'm thinking things like: GitHub repo URL, API keys (Claude/OpenAI), which integrations to enable (Telegram bot token, etc.). Are there other inputs, or do you want Locus to have sensible defaults for everything else (instance size, region, OS)?",
      "timestamp": 1771432573496
    },
    {
      "role": "user",
      "content": "Makes sense to be orchestrator then",
      "timestamp": 1771432648052
    },
    {
      "role": "assistant",
      "content": "Good call. The orchestrator model gives you the best of both worlds — simple UX without the operational burden of being a hosting provider.\n\nSo let's map out the onboarding flow. **What's the minimum set of inputs the user needs to provide to get a working instance?** I'm thinking along the lines of: AWS credentials (IAM role/access keys), GitHub repo URL, LLM API key (Claude/OpenAI), and any integration tokens (like Telegram bot token). Are there other inputs you'd want, or should Locus default everything else — instance size, region, OS, security groups — automatically?",
      "timestamp": 1771432657266
    },
    {
      "role": "user",
      "content": "Makes sense, they should select the integration type, for example Telegram or other integration in the future and should use it for the rest of their usage. LLM API key does not make sense because locus works with claude and codex CLI, which requires manual login with credentials on the server. So I don't know how to manage it",
      "timestamp": 1771433025193
    },
    {
      "role": "assistant",
      "content": "That's a really important point you're raising. Claude CLI and Codex both require interactive authentication — you can't just pass an API key. This means after the instance is provisioned, the user would need to SSH into the server at least once to run `claude login` or authenticate Codex manually. That's a friction point that somewhat undermines the \"one-click\" experience.\n\nThere are a few ways to handle this: (1) provide a simple web-based terminal in the Locus dashboard so the user can authenticate without needing to know SSH, (2) automate the auth by having the user provide their API key in Locus and then inject it into the CLI's config file on the server directly (bypassing the interactive login), or (3) just accept the one-time SSH step and make it as guided as possible with clear instructions.\n\n**Which of these approaches appeals to you, or do you have another idea for bridging this authentication gap?**",
      "timestamp": 1771433037686
    },
    {
      "role": "user",
      "content": "LLM API keys will not work. Therefore, provide a web based terminal in the locus dashboard makes sense",
      "timestamp": 1771433109792
    },
    {
      "role": "assistant",
      "content": "Good choice — a web-based terminal solves the auth problem cleanly without requiring users to know SSH, and it keeps the experience within the Locus dashboard.\n\nNow, thinking about the provisioning itself — **what should happen to the instance if the user's Locus subscription lapses or they stop paying?** Should the instance just keep running (since it's in their AWS account and they pay AWS directly), or should Locus actively shut it down / remove the agent? This matters because if the agent keeps running without Locus orchestration, it could still consume costs but lose access to updates, integration management, etc.",
      "timestamp": 1771433121218
    },
    {
      "role": "user",
      "content": "We can notify the user about the situation and force them somehow to take an action manually",
      "timestamp": 1771433495244
    },
    {
      "role": "assistant",
      "content": "That's a practical approach — since the instance lives in their AWS account, Locus doesn't need to play enforcer. You can send notifications (email, dashboard alerts, even via their Telegram integration) warning them that their Locus orchestration access is expiring, and after a grace period, simply stop managing the instance. The agent would stop receiving updates and integration management, but the instance itself stays in their account — it's their infrastructure, their decision to terminate it.\n\nNow let me ask about something that could become a real operational headache: **How will you handle updates to the Locus agent software on these remote instances?** When you ship a new version of the agent, a new integration, or a security patch, should Locus automatically update the instance in the background, or should the user trigger updates manually from the dashboard? Auto-updates are convenient but risky if something breaks mid-task; manual updates give control but most users will fall behind on versions.",
      "timestamp": 1771433509162
    },
    {
      "role": "user",
      "content": "User will request and update from the dashboard and we will trigger the update",
      "timestamp": 1771433542810
    },
    {
      "role": "assistant",
      "content": "Smart choice — user-triggered updates give you control without the risk of breaking a running agent mid-task. You can show available updates in the dashboard and let users apply them when they're ready.\n\nNow here's a question that ties back to the core experience: **How will users monitor what their agent is doing on the remote instance?** For example, when a Telegram request comes in and the agent starts working, should the user be able to see real-time logs/activity in the Locus dashboard, or is it more of a \"fire and forget\" model where they just see the final PR/commit result? This matters because debugging issues (\"why didn't the agent do what I asked?\") is much harder without visibility into what's happening on the instance.",
      "timestamp": 1771433553324
    },
    {
      "role": "user",
      "content": "The telegram already shows the statuses so we should not worry about it",
      "timestamp": 1771433630407
    },
    {
      "role": "assistant",
      "content": "Good point — since Telegram already provides status updates and activity feedback to the user, they have visibility into what the agent is doing without needing a separate monitoring layer in the dashboard. That keeps the dashboard focused on provisioning and management rather than duplicating what the integration already handles.\n\nHere's a question about security and access: **What level of network access should the provisioned instance have by default?** For example, the agent needs outbound internet access (to reach GitHub, Claude API, Telegram API), but should it have any inbound access beyond what Locus needs for orchestration and the web terminal? Locking down inbound access by default (no public SSH, no open ports) and routing everything through Locus's secure connection would reduce the attack surface significantly — but it also means users can't SSH in directly if they want to. What's your preference here?",
      "timestamp": 1771433644462
    },
    {
      "role": "user",
      "content": "We can password protect the instances on aws level and recommend IP restrictions on dashboard so ssh becomes quite restrictiv",
      "timestamp": 1771433761722
    },
    {
      "role": "assistant",
      "content": "That's a solid approach — password-protected instances with recommended IP restrictions gives users a reasonable security baseline without over-engineering it. Most individual developers will appreciate having SSH available when they need it, and the IP restriction recommendation in the dashboard is a nice touch that educates users on security best practices without forcing lockdown.\n\nNow let's talk about the **cost and pricing angle**: since Locus is orchestrating instances in the user's AWS account, you're not charging for compute — but you *are* providing significant value (provisioning, web terminal, update management, integration setup). **Will this AWS hosting orchestration be part of the existing Locus subscription, or do you see it as a separate add-on / higher tier?** This matters because it affects how you position the feature and whether it's accessible to all users or gated behind a premium plan.",
      "timestamp": 1771433773778
    },
    {
      "role": "user",
      "content": "We can simply make it equal to aws cost itself. Later, we can restrict this access on our application level so that premium users will only have access to it",
      "timestamp": 1771433833853
    },
    {
      "role": "assistant",
      "content": "Interesting pricing approach — matching AWS cost means you're essentially offering the orchestration for free initially, then monetizing it later by gating access behind a premium tier. That's a solid growth strategy: remove all friction now to drive adoption, then capture value once users depend on it.\n\nOne thing to think through carefully: **How will you handle the billing transparency for this?** Since the instance runs in the user's AWS account, they'll see the AWS charges on their own bill — but will Locus also show estimated costs in the dashboard (e.g., \"your instance has cost ~$12.40 this month\") so users aren't surprised? Or do you trust that individual developers on small instances will just manage their own AWS billing? Showing cost estimates could be a nice trust-building feature, but it also means integrating with the AWS Cost Explorer API, which adds complexity.",
      "timestamp": 1771433846811
    },
    {
      "role": "user",
      "content": "Does not AWS show the billing in a transparent way? Users can go to their aws account and check, am I wrong?",
      "timestamp": 1771435272991
    },
    {
      "role": "assistant",
      "content": "You're absolutely right — AWS does show billing transparently in their console. Individual developers on small instances can easily check their own AWS billing dashboard. Adding a cost estimation layer inside Locus would be extra engineering effort for relatively little value at this stage, especially since you're targeting technically capable individual developers who already have AWS accounts.\n\nNow, here's a practical question about the onboarding experience: **What's the minimum AWS permission set Locus needs from the user to provision and manage instances?** For example, you'll need EC2 permissions (launch, stop, terminate instances), VPC/security group management, and possibly IAM permissions to create the instance profile. The narrower the permission scope you request, the more comfortable users will be granting access — but too narrow and you won't be able to automate everything. Have you thought about what specific AWS resources Locus will need to create and manage?",
      "timestamp": 1771435287561
    },
    {
      "role": "user",
      "content": "I think narrow permission set is enough because there is not too much thing we will do. Just launch a server, ssh into it, install some stuff, and run some commands inside the instance. That is it",
      "timestamp": 1771435346737
    },
    {
      "role": "assistant",
      "content": "You're right — for this use case, the permission set is quite narrow. You basically need: `ec2:RunInstances`, `ec2:DescribeInstances`, `ec2:StartInstances`, `ec2:StopInstances`, `ec2:TerminateInstances`, and some security group permissions. That's a very tight scope, which makes the IAM setup straightforward and builds user trust since they can see exactly what Locus can do in their account.\n\nNow, here's a practical question about the actual provisioning flow: **What base image/AMI will Locus use for these instances?** You'll need a consistent environment with Node.js (for Locus), Git, Claude CLI, Codex, and any other dependencies pre-installed. Are you thinking of maintaining a custom AMI that you keep updated, or starting from a standard Ubuntu AMI and running the installation script on first boot (which is what users already do manually today)? A custom AMI means faster provisioning (everything pre-installed) but adds the maintenance burden of keeping the image updated.",
      "timestamp": 1771435360106
    },
    {
      "role": "user",
      "content": "Hmm, is there a cost difference for these custom instances? If no we can continue with it",
      "timestamp": 1771435399824
    },
    {
      "role": "assistant",
      "content": "No, there's no meaningful cost difference for custom AMIs. AWS charges you for the EC2 instance compute and EBS storage, not for the AMI itself (storing the AMI snapshot has a negligible cost — pennies per month). So a custom AMI with everything pre-installed costs the same to run as a vanilla Ubuntu AMI, but provisions much faster since you skip the installation step entirely.\n\nThat said, the trade-off is **maintenance**: every time you update the Locus agent, add a new integration, or upgrade Node.js/Claude CLI, you need to rebuild and publish a new AMI version. But since you've already decided on user-triggered updates, you could use a hybrid approach — start from a custom AMI for fast initial provisioning, then apply incremental updates via the dashboard update mechanism after that.\n\nNow here's a question about **multi-region support**: AWS instances are region-specific, and latency to GitHub/Telegram APIs can vary by region. **Will you let users choose their AWS region, or will Locus default to a single region (e.g., us-east-1)?** Defaulting to one region keeps things simple, but if you have users in Europe or Asia, they might prefer a closer region for lower latency and to comply with data residency preferences.",
      "timestamp": 1771435419616
    },
    {
      "role": "user",
      "content": "Default is fine",
      "timestamp": 1771435430231
    },
    {
      "role": "assistant",
      "content": "Good call — defaulting to a single region like `us-east-1` keeps the provisioning logic simple and avoids the complexity of managing AMIs across multiple regions. You can always add region selection later if user demand warrants it.\n\nWe've covered a lot of ground here — target user, instance lifecycle, orchestrator model, authentication, security, pricing, permissions, AMI strategy, and region. There's one more practical question I want to ask before we wrap up:\n\n**How will the user connect their GitHub repo to the provisioned instance?** Will Locus clone the repo during provisioning using a GitHub personal access token or SSH key the user provides, or will the user set up the repo connection themselves via the web terminal after the instance is live? This affects both the onboarding flow and how the agent pushes commits back to GitHub.",
      "timestamp": 1771435441075
    },
    {
      "role": "user",
      "content": "We already have a way on installation where user provides the github access token. I think this is ideal",
      "timestamp": 1771435473987
    },
    {
      "role": "assistant",
      "content": "That's great — reusing the existing GitHub access token flow during installation means the repo connection is already solved. The provisioning script can clone the repo automatically using the token the user already provided, and the agent can push commits back using the same credentials. No new UX needed for this step.\n\nI think we've covered the major architectural and product decisions pretty thoroughly at this point — target user, instance model, orchestrator approach, auth, security, pricing, permissions, AMI strategy, region, updates, monitoring, and repo connection.\n\n**Would you like me to wrap this up and generate a comprehensive summary artifact of all the decisions and requirements we've discussed?** Or is there another aspect of this feature — like the actual dashboard UI flow, the Terraform/CloudFormation approach for provisioning, or how you'd handle edge cases (instance crashes, disk full, etc.) — that you'd like to explore before we close out?",
      "timestamp": 1771435488101
    },
    {
      "role": "user",
      "content": "We definitely need to handle them, but maybe in the future. On initial step, not very crucial",
      "timestamp": 1771435537453
    },
    {
      "role": "assistant",
      "content": "That's a pragmatic approach — edge cases like instance crashes, disk full, and recovery scenarios are important but can absolutely be deferred to a later iteration. For an MVP targeting individual developers, the core provisioning and management flow is what matters most.\n\nI think we've thoroughly explored this feature. We've nailed down decisions on: target user (individual devs), instance model (persistent, always-on), business model (orchestrator, not hosting provider), authentication (web terminal for CLI login), security (password + IP restrictions), pricing (at-cost initially, premium-gated later), provisioning (custom AMI, single region, minimal AWS permissions), updates (user-triggered), monitoring (via Telegram), and GitHub integration (existing token flow).\n\n**Would you like me to generate a comprehensive summary artifact capturing all these decisions and the overall architecture?** This would serve as the product requirements document for when you're ready to start building this feature.",
      "timestamp": 1771435549390
    },
    {
      "role": "user",
      "content": "Yeah lets do it",
      "timestamp": 1771435788202
    },
    {
      "role": "assistant",
      "content": "The comprehensive summary has been saved to `.locus/artifacts/aws-instance-orchestration-prd.md`. It covers everything we discussed:\n\n- **Problem statement** and the pain points you're solving\n- **Architecture decisions** — orchestrator model, persistent instances, custom AMI, single region, minimal AWS permissions\n- **User experience flows** — onboarding, web terminal for CLI auth, security, monitoring via Telegram, user-triggered updates\n- **Pricing strategy** — at-cost initially, premium-gated later\n- **Subscription lifecycle** — graceful degradation on lapse\n- **Deferred items** — edge cases, team support, multi-region, auto-updates, other cloud providers\n- **Technical considerations** — AWS SDK, xterm.js for web terminal, dashboard features needed\n- **Full decision summary table** for quick reference\n\nThis artifact should serve as your product requirements document when you're ready to start building the feature. You can reference it for implementation planning and share it with collaborators to get everyone aligned.",
      "timestamp": 1771435853516
    }
  ],
  "insights": [
    {
      "id": "ins-1771432372589-hfupk",
      "type": "requirement",
      "title": "One-click agent server deployment",
      "content": "Users need to be able to launch an agent server without needing cloud infrastructure knowledge (AWS, Azure, etc.). The current installation script exists but still requires users to provision and configure their own servers.",
      "tags": ["infrastructure", "ux", "deployment"],
      "createdAt": "2026-02-18T16:32:52.589Z"
    },
    {
      "id": "ins-1771432372589-ffx7g",
      "type": "learning",
      "title": "Current pain points in self-hosting",
      "content": "The current workflow requires users to: 1) Launch a cloud instance manually, 2) Clone their working repo, 3) Install Claude/Codex, 4) Manage integration versions. Even with an installation script, this is too complex for users without cloud experience.",
      "tags": ["user-research", "pain-points"],
      "createdAt": "2026-02-18T16:32:52.589Z"
    },
    {
      "id": "ins-1771432413712-ofxus",
      "type": "decision",
      "title": "Initial target: individual developers",
      "content": "The first version of the managed hosting feature will target individual developers, not teams or companies. Team/shared service use cases will come later.",
      "tags": ["target-audience", "mvp", "scope"],
      "createdAt": "2026-02-18T16:33:33.712Z"
    },
    {
      "id": "ins-1771432511445-3gt20",
      "type": "decision",
      "title": "Persistent always-on instances",
      "content": "Agent servers will run as persistent, always-on instances rather than ephemeral/serverless. This is necessary because the repo must stay cloned locally, integrations need to remain configured, and the agent must continuously listen for incoming requests (Telegram, etc.). A small instance like t3.micro/t3.small keeps costs reasonable at ~$8-15/month for individual developers.",
      "tags": ["architecture", "aws", "cost", "hosting"],
      "createdAt": "2026-02-18T16:35:11.445Z"
    },
    {
      "id": "ins-1771432573495-27bhw",
      "type": "idea",
      "title": "Infrastructure orchestrator model over hosting provider",
      "content": "Locus should orchestrate instance provisioning in the user's own AWS account rather than hosting instances on Locus-owned infrastructure. This avoids becoming a hosting provider (billing complexity, abuse risk, uptime liability) while still enabling a one-click experience. Users pay AWS directly, keep their code/keys in their own account, and connect via IAM role or access keys.",
      "tags": ["architecture", "business-model", "aws", "hosting"],
      "createdAt": "2026-02-18T16:36:13.495Z"
    },
    {
      "id": "ins-1771432657265-2wc6p",
      "type": "decision",
      "title": "Infrastructure orchestrator model confirmed",
      "content": "Locus will act as an infrastructure orchestrator, provisioning instances in the user's own AWS account rather than hosting on Locus-owned infrastructure. Users connect their AWS account via IAM role or access keys, and Locus handles all provisioning and setup transparently.",
      "tags": ["architecture", "aws", "hosting-model"],
      "createdAt": "2026-02-18T16:37:37.265Z"
    },
    {
      "id": "ins-1771433037685-629nh",
      "type": "concern",
      "title": "LLM CLI authentication requires manual server login",
      "content": "Claude CLI and Codex require interactive credential-based login (not just an API key). This means users must SSH into the provisioned instance at least once to authenticate, which adds friction to the onboarding flow and partially breaks the one-click deployment promise.",
      "tags": ["authentication", "onboarding", "ux", "blocker"],
      "createdAt": "2026-02-18T16:43:57.685Z"
    },
    {
      "id": "ins-1771433121217-ezrdg",
      "type": "decision",
      "title": "Web-based terminal for LLM CLI authentication",
      "content": "Locus will provide a web-based terminal embedded in the dashboard so users can authenticate Claude CLI and Codex interactively on the provisioned instance without needing SSH knowledge. This preserves the managed experience while accommodating CLIs that require interactive login rather than API key injection.",
      "tags": ["onboarding", "authentication", "ux", "dashboard"],
      "createdAt": "2026-02-18T16:45:21.217Z"
    },
    {
      "id": "ins-1771433509160-dkoz5",
      "type": "decision",
      "title": "Graceful degradation on subscription lapse",
      "content": "When a user's Locus subscription lapses, Locus will notify the user and stop managing/orchestrating the instance after a grace period. The instance continues running in the user's AWS account but loses Locus-managed updates and integration management. Users must take manual action to either re-subscribe or terminate the instance themselves.",
      "tags": ["billing", "lifecycle", "subscription"],
      "createdAt": "2026-02-18T16:51:49.160Z"
    },
    {
      "id": "ins-1771433553323-pa3za",
      "type": "decision",
      "title": "User-triggered updates from dashboard",
      "content": "Locus agent software updates on remote instances will be user-initiated through the dashboard, not automatic. When a new version is available, the user sees it in the dashboard and triggers the update manually. This avoids the risk of auto-updates breaking an agent mid-task while keeping instances up to date.",
      "tags": ["updates", "deployment", "dashboard"],
      "createdAt": "2026-02-18T16:52:33.323Z"
    },
    {
      "id": "ins-1771433644461-qwgrx",
      "type": "decision",
      "title": "Telegram handles agent activity monitoring",
      "content": "Real-time agent activity monitoring will be handled through the existing Telegram integration (or future messaging integrations), which already shows task statuses. The Locus dashboard does not need a separate real-time log/activity view for agent execution — it focuses on provisioning, updates, and instance management.",
      "tags": ["monitoring", "telegram", "dashboard-scope"],
      "createdAt": "2026-02-18T16:54:04.461Z"
    },
    {
      "id": "ins-1771433773777-19bm1",
      "type": "decision",
      "title": "Instance security: password + IP restriction recommendations",
      "content": "Provisioned AWS instances will be password-protected at the AWS level. The Locus dashboard will recommend IP restrictions to users, making SSH access restrictive. This provides a reasonable security baseline for individual developers while keeping direct SSH access available when needed.",
      "tags": ["security", "aws", "instance-management"],
      "createdAt": "2026-02-18T16:56:13.777Z"
    },
    {
      "id": "ins-1771433846810-p05fk",
      "type": "decision",
      "title": "Pricing: at-cost initially, premium-gated later",
      "content": "The AWS hosting orchestration will initially be priced at AWS cost (no markup), making it essentially free orchestration. Later, access to this feature will be restricted to premium/paid tier users only, creating a conversion lever once users depend on the managed hosting experience.",
      "tags": ["pricing", "monetization", "growth-strategy"],
      "createdAt": "2026-02-18T16:57:26.810Z"
    },
    {
      "id": "ins-1771435287560-q1rby",
      "type": "decision",
      "title": "No in-app cost estimation — rely on AWS billing",
      "content": "Locus will not build a cost estimation or billing transparency feature in the dashboard. Users will rely on AWS's own billing console to track instance costs. This avoids the complexity of integrating with AWS Cost Explorer API and is sufficient for the individual developer target audience.",
      "tags": ["billing", "dashboard", "scope"],
      "createdAt": "2026-02-18T17:21:27.560Z"
    },
    {
      "id": "ins-1771435360105-s72bw",
      "type": "decision",
      "title": "Minimal AWS permissions scope",
      "content": "Locus will request a narrow set of AWS permissions limited to EC2 instance lifecycle management (launch, start, stop, terminate, describe) and basic security group configuration. No broad IAM, S3, or other service permissions needed. This keeps the IAM role/policy simple and builds user trust through transparency.",
      "tags": ["aws", "security", "permissions", "onboarding"],
      "createdAt": "2026-02-18T17:22:40.105Z"
    },
    {
      "id": "ins-1771435419615-vnef0",
      "type": "decision",
      "title": "Custom AMI for fast provisioning",
      "content": "Locus will maintain a custom AMI with all dependencies pre-installed (Node.js, Git, Claude CLI, Codex, Locus agent) for fast instance provisioning. There is no meaningful cost difference versus a standard Ubuntu AMI. Incremental updates after initial provisioning will be handled through the dashboard update mechanism.",
      "tags": ["aws", "provisioning", "ami", "infrastructure"],
      "createdAt": "2026-02-18T17:23:39.615Z"
    },
    {
      "id": "ins-1771435441074-jpxl2",
      "type": "decision",
      "title": "Default single AWS region",
      "content": "Locus will default to a single AWS region (e.g., us-east-1) for instance provisioning rather than offering multi-region selection. This simplifies AMI management and provisioning logic. Multi-region support can be added later based on user demand.",
      "tags": ["infrastructure", "aws", "provisioning"],
      "createdAt": "2026-02-18T17:24:01.074Z"
    },
    {
      "id": "ins-1771435488100-77yh9",
      "type": "decision",
      "title": "GitHub repo connection via existing access token flow",
      "content": "The provisioned instance will connect to the user's GitHub repo using the GitHub personal access token already collected during the existing Locus installation/onboarding flow. This token handles both cloning the repo during provisioning and pushing commits back. No new authentication mechanism is needed for GitHub integration.",
      "tags": ["github", "authentication", "onboarding", "provisioning"],
      "createdAt": "2026-02-18T17:24:48.100Z"
    },
    {
      "id": "ins-1771435549389-4lws1",
      "type": "decision",
      "title": "Edge cases deferred to future iterations",
      "content": "Handling edge cases such as instance crashes, disk full, and recovery scenarios will be deferred to future iterations. The initial version will focus on the core provisioning, management, and update flow for individual developers.",
      "tags": ["scope", "mvp", "prioritization"],
      "createdAt": "2026-02-18T17:25:49.389Z"
    }
  ],
  "createdAt": "2026-02-18T16:29:08.698Z",
  "updatedAt": "2026-02-18T17:30:53.516Z",
  "metadata": {
    "model": "opus",
    "provider": "claude"
  }
}
